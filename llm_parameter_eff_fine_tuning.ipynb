{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter efficient fine tuning with LoRA\n",
    "\n",
    "### The goal of this project is to first evaluate a pretrained DistilBERT model on the MultiNLI dataset, and then parameter efficient fine tune it leveraging Low Rank Adaptation (LoRA)\n",
    "\n",
    "The MultiNLI dataset is a crowd-sourced collection of 433k sentence pairs labeled for entailment, contradiction, or neutrality.  It was also used in the RepEval 2017 shared task at EMNLP. Each data point in MultiNLI has a premise and a hypothesis, and the task is to figure out the relationship between the two. The label can be one of three types:\n",
    "\n",
    "Entailment (0) â€“ the hypothesis clearly follows from the premise\n",
    "\n",
    "Neutral (1) â€“ the hypothesis could be true, but itâ€™s not certain\n",
    "\n",
    "Contradiction (2) â€“ the hypothesis directly contradicts the premise\n",
    "\n",
    "Itâ€™s a straightforward three-class classification task, but still a good challenge for testing how well a model understands language and reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pytorch_env_310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType, AutoPeftModelForSequenceClassification, PeftModel\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import os\n",
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Read in the dataset and display the size\n",
    "\n",
    "In the MultiNLI dataset, there are 2 types of validation sets that can be used:\n",
    "\n",
    "Validation Matched: This split contains validation examples from the same genres as those seen during training (e.g., government, fiction).\n",
    "\n",
    "Validation Mismatched: This split includes examples from different genres that were not used in training (e.g., telephone conversation, travel guides).\n",
    "\n",
    "The idea is to test both in-domain (matched) and out-of-domain (mismatched) generalizationâ€”so you can see not just how well your model performs on familiar styles of text, but also how it handles new, unseen ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 392702\n",
      "Validation (matched) dataset size: 9815\n",
      "Validation (mismatched) dataset size: 9832\n"
     ]
    }
   ],
   "source": [
    "# Load the MultiNLI dataset\n",
    "dataset = load_dataset(\"multi_nli\")\n",
    "\n",
    "print(\"Train dataset size:\", len(dataset[\"train\"]))\n",
    "print(\"Validation (matched) dataset size:\", len(dataset[\"validation_matched\"]))\n",
    "print(\"Validation (mismatched) dataset size:\", len(dataset[\"validation_mismatched\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Show a few example rows from the training set. \n",
    "\n",
    "Recapping labels:\n",
    "Entailment (0) â€“ the hypothesis clearly follows from the premise\n",
    "\n",
    "Neutral (1) â€“ the hypothesis could be true, but itâ€™s not certain\n",
    "\n",
    "Contradiction (2) â€“ the hypothesis directly contradicts the premise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label: 2 (Contradiction)\n",
      "Premise: so i i know the people at TI who are doing this and i heard about it so i called them and ask if i could could participate and uh\n",
      "Hypothesis: I don't know anyone at TI and that's okay because I don't want to participate anyway.\n",
      "\n",
      "Label: 1 (Neutral)\n",
      "Premise: We also show how the advocate component of statewide websites promotes effective representation by sharing legal resources and expertise - generally a function of legal work supervisors.\n",
      "Hypothesis: Legal work supervisors are usually tasked with sharing legal advice and resources.\n",
      "\n",
      "Label: 0 (Entailment)\n",
      "Premise: The New Territories can be explored by taking the Kowloon Canton Railway (KCR), which makes 10 stops between the station in Kowloon and Sheung Shui, the last stop before entering China.\n",
      "Hypothesis: The Kowloon Canton Railway makes over five stops between the station and Sheung Shui.\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"Entailment\", 1: \"Neutral\", 2: \"Contradiction\"}\n",
    "\n",
    "def show_one_example_per_label(dataset_split, offset=0):\n",
    "    seen_labels = set()\n",
    "    subset = dataset_split.select(range(offset, len(dataset_split)))\n",
    "    \n",
    "    for example in subset:\n",
    "        label = example[\"label\"]\n",
    "        if label in [0, 1, 2] and label not in seen_labels:\n",
    "            print(f\"\\nLabel: {label} ({label_map[label]})\")\n",
    "            print(\"Premise:\", example[\"premise\"])\n",
    "            print(\"Hypothesis:\", example[\"hypothesis\"])\n",
    "            seen_labels.add(label)\n",
    "        if len(seen_labels) == 3:\n",
    "            break\n",
    "\n",
    "# Load and use\n",
    "dataset = load_dataset(\"multi_nli\")\n",
    "show_one_example_per_label(dataset[\"train\"], offset=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define the model name and load the tokenizer\n",
    "\n",
    "Why this model and tokenizer?\n",
    "\n",
    "The distilbert-base-uncased model and its tokenizer are well-suited for this task for several reasons:\n",
    "\n",
    "1) Lightweight yet effective: As a distilled version of BERT, DistilBERT retains about 97% of BERTâ€™s performance on tasks like natural language inference (NLI), while being 40% smaller and 60% faster. This makes it an efficient choice for fine-tuning without compromising much on accuracy.\n",
    "\n",
    "2) Uncased variant: The model treats uppercase and lowercase letters the same, which reduces vocabulary size and simplifies training. This is particularly useful for NLI, where case sensitivity is typically not essential.\n",
    "\n",
    "3) Seamless integration with Hugging Face tools: It is fully compatible with Hugging Face's transformers library, including AutoTokenizer and AutoModelForSequenceClassification, making setup and fine-tuning straightforward.\n",
    "\n",
    "4) Pretrained on diverse data: The model has been pretrained on a large and varied text corpus (including Wikipedia and BookCorpus), providing a strong foundation for transfer learning on datasets like MultiNLI.\n",
    "\n",
    "5) Resource-friendly: With fewer parameters than BERT, DistilBERT runs efficiently on limited hardware, making it ideal for quick experiments, smaller machines, or educational setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocess the data\n",
    "\n",
    "This step gets the dataset ready for training by tokenizing the premiseâ€“hypothesis pairs using the chosen tokenizer. It takes care of padding, truncation, and renames the label to labels, which is what the Hugging Face Trainer expects. The .map() function applies this to the full dataset, and we remove all original columns since theyâ€™re no longer needed for training â€” this keeps the dataset clean and avoids passing unnecessary data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 17158, 2135, 6949, 8301, 25057, 2038, 2048, 3937, 9646, 1011, 4031, 1998, 10505, 1012, 102, 4031, 1998, 10505, 2024, 2054, 2191, 6949, 8301, 25057, 2147, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    # Use 'labels' key as Trainer expects this key\n",
    "    tokenized_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Map the function over the dataset, remove columns\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Print a sample of the tokenized dataset\n",
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Downsample the dataset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset size: 50000\n",
      "Eval subset size: 4000\n"
     ]
    }
   ],
   "source": [
    "# Downsample the dataset for faster training\n",
    "train_dataset = tokenized_datasets[\"train\"].select(range(50000))\n",
    "eval_dataset = tokenized_datasets[\"validation_matched\"].select(range(4000))\n",
    "\n",
    "print(\"Train subset size:\", len(train_dataset))\n",
    "print(\"Eval subset size:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "base_model = base_model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu')) # Leverage cuda if gpu is available, else use cpu\n",
    "\n",
    "# Display the pretrained model\n",
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On inspecting the model output above, it is seen that the model follows the standard DistilBertForSequenceClassification setup. It has 6 transformer layers (half of BERTâ€™s), which keeps it compact without losing too much performance. The embedding layer uses a vocab size of 30,522 (so it can handle that many unique tokens) and positional encodings up to 512 tokens to help the model keep track of word order.\n",
    "\n",
    "On top of the base encoder, thereâ€™s a simple classification head â€” dropout, a linear \"pre-classifier\", and a final layer that maps to 3 output classes for the MultiNLI task. It uses GELU as the activation function, which is smoother than ReLU and tends to work better in transformers. Dropout and LayerNorm are used throughout for regularization and training stability.\n",
    "\n",
    "Overall, it's a lightweight, efficient setup thatâ€™s a great fit for NLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluate the pre-trained model\n",
    "\n",
    "\n",
    "In this step, the modelâ€™s performance is evaluated using macro-averaged F1 score, precision, and recall. The predicted class for each example is obtained by applying argmax to the modelâ€™s output logits, and these predictions are compared to the ground truth labels. Macro averaging ensures that all classes are treated equally, which is important for a dataset like MultiNLI where the label distribution may not be perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute macro-averaged F1 score, precision, and recall for classification predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eval_pred : Tuple[np.ndarray, np.ndarray]\n",
    "        A tuple containing:\n",
    "        - predictions: array-like of shape (n_samples, n_classes), raw model outputs (logits).\n",
    "        - labels: array-like of shape (n_samples,), ground truth class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary with macro-averaged evaluation metrics:\n",
    "        - \"f1_score\": float\n",
    "        - \"precision\": float\n",
    "        - \"recall\": float\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(predictions), dim=-1).cpu()\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    precision = precision_score(labels, preds, average='macro')\n",
    "    recall = recall_score(labels, preds, average='macro')\n",
    "    return {\"f1_score\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step runs a quick evaluation of the base DistilBERT model before any fine-tuning, just to get a baseline on how well it performs out of the box. The Trainer is set up to skip training and only do evaluation on the MultiNLI validation set.\n",
    "\n",
    "The batch size is kept small to avoid memory issues, and logging is minimal since the goal here is just to get a reference point. Even though evaluation_strategy=\"epoch\" isnâ€™t really needed without training, it keeps things consistent with the rest of the pipeline.\n",
    "\n",
    "Running this helps show how much the pretrained model already understands the task â€” and gives something solid to compare against once fine-tuning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pytorch_env_310\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\annmo\\AppData\\Local\\Temp\\ipykernel_37620\\3654479608.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_base = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pretrained base model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0992169380187988, 'eval_model_preparation_time': 0.001, 'eval_f1_score': 0.195431192931623, 'eval_precision': 0.32689638378180935, 'eval_recall': 0.3362731580642897, 'eval_runtime': 45.1009, 'eval_samples_per_second': 88.69, 'eval_steps_per_second': 11.086}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the pretrained base model\n",
    "training_args_base = TrainingArguments(\n",
    "    output_dir=\"./base_model_eval\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_base = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args_base,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Evaluating pretrained base model...\")\n",
    "results_base = trainer_base.evaluate()\n",
    "print(results_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Confirm the model is running on cuda\n",
    "print(\"Model is running on:\", next(trainer_base.model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Implement parameter efficient fine-tuning with Low Rank Adaptation (LoRA)\n",
    "\n",
    "This step sets up LoRA to fine-tune only a small, targeted subset of the base DistilBERT model, making training more efficient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Define target modules from DIstilBERT for LoRA\n",
    "The target_modules focus on the query, key, value, and output projection layers of the self-attention block in the last transformer layer (layer.5). Since attention layers play a key role in how the model processes sentence pairs, modifying them directly makes sense for a task like NLI â€” and narrowing it to just one layer keeps the update lightweight.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = [\n",
    "    \"distilbert.transformer.layer.5.attention.q_lin\",\n",
    "    \"distilbert.transformer.layer.5.attention.k_lin\",\n",
    "    \"distilbert.transformer.layer.5.attention.v_lin\",\n",
    "    \"distilbert.transformer.layer.5.attention.out_lin\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Define the LoRA configuration\n",
    "\n",
    "The LoraConfig defines how LoRA will be applied. The rank r=64 and lora_alpha=16 control the low-rank adaptation strength, while a small dropout (0.05) adds regularization. use_rslora=True enables a more memory-efficient variant of LoRA (rank-stabilized LoRA), which is helpful when working with limited GPU resources.\n",
    "\n",
    "bias=\"none\" means LoRA skips modifying bias terms â€” a common choice to keep things simple unless there's a strong reason to include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=64,  \n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules= target_modules,  \n",
    "    bias=\"none\",\n",
    "    use_rslora=True,  \n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Apply PEFT\n",
    "\n",
    "The model is wrapped with get_peft_model to inject the LoRA layers, and itâ€™s moved to the GPU if available. Finally, printing the trainable parameters confirms that only a small fraction of the full model is being updated â€” a big win in terms of speed and memory, without compromising much on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 986,115 || all params: 67,941,894 || trainable%: 1.4514\n"
     ]
    }
   ],
   "source": [
    "model = base_model\n",
    "# Apply PEFT\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Run PEFT\n",
    "\n",
    "This setup fine-tunes the PEFT (LoRA-injected) model on the MultiNLI dataset using Hugging Faceâ€™s Trainer, with training arguments tuned for efficiency and stability â€” especially when running on a GPU:\n",
    "\n",
    "1) evaluation_strategy=\"epoch\" and save_strategy=\"epoch\" ensure the model is evaluated and checkpointed at the end of every epoch. Since training on a GPU is relatively fast, epoch-level evaluation provides a good balance between performance tracking and speed.\n",
    "\n",
    "1) load_best_model_at_end=True automatically restores the best checkpoint based on validation loss, which is especially useful when training for many epochs â€” it saves having to manually track which checkpoint did best.\n",
    "\n",
    "3) learning_rate=2e-5 is a solid starting point for fine-tuning with LoRA, and works well with GPU-accelerated training where updates are fast and stable.\n",
    "\n",
    "4) num_train_epochs=12 is intentionally set a bit high â€” LoRA only updates a small number of parameters, so more epochs are usually needed for the model to fully adapt.\n",
    "\n",
    "5) weight_decay=0.01 adds light regularization, helping avoid overfitting even on high-capacity hardware like GPUs.\n",
    "\n",
    "6) per_device_train_batch_size=8 and per_device_eval_batch_size=8 are conservative and help avoid OOM errors. On a GPU, this keeps training smooth without pushing the limits â€” especially useful if other processes are sharing the GPU.\n",
    "\n",
    "7) dataloader_num_workers=10 helps fully utilize the CPU to keep the GPU fed with data, reducing bottlenecks in the training loop.\n",
    "\n",
    "8) Logging every 10 steps gives frequent updates without spamming, and report_to=\"none\" keeps the run lightweight unless logging integrations are explicitly needed.\n",
    "\n",
    "Running on a GPU makes this setup more efficient overall, but the structure still prioritizes stability and good generalization â€” which is especially important in a PEFT workflow where youâ€™re only fine-tuning a small part of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annmo\\AppData\\Local\\Temp\\ipykernel_37620\\776083117.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75000' max='75000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75000/75000 3:10:35, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.869497</td>\n",
       "      <td>0.597022</td>\n",
       "      <td>0.599923</td>\n",
       "      <td>0.597384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.823980</td>\n",
       "      <td>0.623062</td>\n",
       "      <td>0.625834</td>\n",
       "      <td>0.623797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.841700</td>\n",
       "      <td>0.798245</td>\n",
       "      <td>0.644088</td>\n",
       "      <td>0.644385</td>\n",
       "      <td>0.644189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.755100</td>\n",
       "      <td>0.794242</td>\n",
       "      <td>0.646046</td>\n",
       "      <td>0.648953</td>\n",
       "      <td>0.646444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.781500</td>\n",
       "      <td>0.785159</td>\n",
       "      <td>0.651598</td>\n",
       "      <td>0.651919</td>\n",
       "      <td>0.651671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.725300</td>\n",
       "      <td>0.771869</td>\n",
       "      <td>0.659214</td>\n",
       "      <td>0.659477</td>\n",
       "      <td>0.659287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.773765</td>\n",
       "      <td>0.658147</td>\n",
       "      <td>0.659780</td>\n",
       "      <td>0.658630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.772650</td>\n",
       "      <td>0.661238</td>\n",
       "      <td>0.663629</td>\n",
       "      <td>0.661598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.770835</td>\n",
       "      <td>0.656224</td>\n",
       "      <td>0.660470</td>\n",
       "      <td>0.657407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.767022</td>\n",
       "      <td>0.661701</td>\n",
       "      <td>0.664589</td>\n",
       "      <td>0.662606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.764254</td>\n",
       "      <td>0.666222</td>\n",
       "      <td>0.667671</td>\n",
       "      <td>0.666719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.750900</td>\n",
       "      <td>0.763120</td>\n",
       "      <td>0.667601</td>\n",
       "      <td>0.668778</td>\n",
       "      <td>0.667997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75000, training_loss=0.786420759938558, metrics={'train_runtime': 11464.7455, 'train_samples_per_second': 52.334, 'train_steps_per_second': 6.542, 'total_flos': 8.12994637824e+16, 'train_loss': 0.786420759938558, 'epoch': 12.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./peft_multi_nli_results\",\n",
    "    evaluation_strategy=\"epoch\",   \n",
    "    save_strategy=\"epoch\",         \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=10,\n",
    ")\n",
    "\n",
    "# Prepare the Trainer for PEFT training\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.label_names = [\"labels\"]\n",
    "\n",
    "# Fine-tune the PEFT (LoRA) model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training shows a steady improvement across the 12 epochs. The validation F1 score starts around 0.59 and climbs to ~0.665 by the end, with precision and recall tracking closely â€” suggesting consistent gains in overall performance.\n",
    "\n",
    "The validation loss gradually decreases, which is a good sign that the model is generalizing better over time, without major overfitting. Even though thereâ€™s some fluctuation in training loss (as expected with small batches and LoRA), the upward trend in F1 is clear.\n",
    "\n",
    "Notably, the biggest jumps in performance happen in the first 6â€“7 epochs, after which the gains start to level off â€” so in a future run, early stopping around epoch 10 might be a good option. Overall, the model shows solid learning dynamics and benefits from longer training, which aligns with expectations for PEFT setups where only a small subset of parameters is being updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save the PEFT adapters for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT adapters saved to ./peft_multi_nli\n"
     ]
    }
   ],
   "source": [
    "peft_model.save_pretrained(\"./peft_multi_nli\")\n",
    "print(\"PEFT adapters saved to ./peft_multi_nli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With training complete, the LoRA-adapted model shows consistent gains in performance over the base model, especially in terms of F1 score and overall stability across epochs. By saving only the adapter weights instead of the full model, the result is a lightweight, modular checkpoint thatâ€™s easy to share or reuse â€” while still capturing all the task-specific learning. This setup makes fine-tuning both efficient and practical, especially for larger models or when working with limited compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Try QLoRA, AdaLoRA\n",
    "\n",
    "2) Use larger models like RoBERTa or BERT-large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
